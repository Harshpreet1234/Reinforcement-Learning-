import numpy as np

# Define the environment
n_states = 6
n_actions = 2
reward_matrix = np.array([
    [-1, -1, -1, -1, 0, -1],
    [-1, -1, -1, 0, -1, 100],
    [-1, -1, -1, 0, -1, -1],
    [-1, 0, 0, -1, 0, -1],
    [0, -1, -1, 0, -1, 100],
    [-1, 0, -1, -1, 0, 100]
])

# Define Q-learning parameters
learning_rate = 0.8
discount_factor = 0.95
n_episodes = 1000

# Initialize the Q-table
Q = np.zeros((n_states, n_actions))

# Q-learning algorithm
for episode in range(n_episodes):
    state = np.random.randint(0, n_states)
    while state != 5:  # Continue until reaching the goal state (state 5)
        action = np.random.randint(0, n_actions)
        next_state = np.argmax(Q[state, :])
        Q[state, action] = (1 - learning_rate) * Q[state, action] + \
                           learning_rate * (reward_matrix[state, action] + discount_factor * Q[next_state, action])
        state = next_state

# Print the learned Q-table
print("Learned Q-table:")
print(Q)

# Testing the learned policy
current_state = 2  # Starting state
path = [current_state]

while current_state != 5:
    action = np.argmax(Q[current_state, :])
    next_state = np.argmax(Q[current_state, :])
    path.append(next_state)
    current_state = next_state

print("Optimal path:", path)
